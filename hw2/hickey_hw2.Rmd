---
title: "Assignment 2"
author: "Jimmy Hickey"
date: "9/14/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

```


**1. Assume $Y_{1} , \dots Y_{n} \mid \theta \sim \text{Normal}(0, \theta)$ and variance parameter has prior $\theta \mid \text{Gamma}(a,b)$.**


**(a) Derive the posterior distribution of $\theta$, i.e. give a parametric family like $\theta \mid Y \sim \text{Beta}(Y, a^{b})$.**

\begin{align*}
p(\theta \mid Y_{1}, \dots , Y_{n}) & = f(\pmb{Y} \mid \theta) \pi(\theta) \\
	& \propto \theta^{a-1} \exp{-b\theta} \cdot  \theta^{-\frac{ n }{ 2 }} \exp \Big\{ -\frac{ 1 }{ 2 \theta } \sum_{i=1}^{n} (Y_{i}^{2}) \Big\} \\
	& = \theta^{(a - \frac{n }{ 2 }) - 1} \exp \Big\{ - b \theta -\frac{ 1 }{ 2 \theta } \sum_{i=1}^{n} (Y_{i}^{2}) \Big\} \\
	& = \theta^{(a - \frac{n }{ 2 }) - 1} \exp \Big[ -\frac{ 1 }{ 2 } \Big\{ 2 b \theta + \frac{ 1 }{ \theta } \sum_{i=1}^{n} (Y_{i}^{2}) \Big\} \Big] \\
	& \Rightarrow \theta \mid Y_{1}, \dots Y_{n} \sim \text{Generalized Inverse Gaussian}\Big(2b, \sum_{i=1}^{n} (Y_{i})^{2}, a - \frac{ n }{ 2 }\Big)
\end{align*}

**(b) Would you say this prior is conjugate? Justify your answer.**

This is not conjugate because it is not a Gamma distribution.



\newpage



**2. Say $\pmb{Y} = (Y_{1}, \dots Y_{p}) \mid \theta \sim \text{Multinomial}(n, \theta)$ for $\theta = (\theta_{1}, \dots , \theta_{p})$ so that the likelihood is**

$$
f(\pmb{Y}\mid \theta) = \frac{n!  }{  \prod_{j=1}^{p} Y_{j}! } \prod_{j=1}^{p} \theta_{j}^{Y_{j}}. 
$$


**(a) Derive the Jeffreys prior for $\theta$.**

We need the Fisher information from the log likelihood function. Also note that we have the restriction that $\sum_{i=1}^{p} \theta_{i} = 1$, 

\begin{align*}
\log\{f(\pmb{Y} \mid \theta)\} & \propto \sum_{i=1}^{p} Y_{i} \log(\theta_{i}) \\
	& = \propto \sum_{i=1}^{p-1} Y_{i} \log(\theta_{i}) + Y_{p} \log(1-\sum_{i=1}^{p-1} \theta_{i}).
\end{align*}

Now we take derivatives,

\begin{align*}
\frac{ \partial \ell }{\partial \theta_{i}} & = \frac{ Y_{i} }{ \theta_{i} } - \frac{ Y_{p} }{ 1-\sum_{i=1}^{p-1} \theta_{i}  } \\ \\
%
\frac{ \partial^{2} \ell }{\partial \theta_{i}^{2} } & = -\frac{ -Y_{i} }{ \theta_{i}^{2} } - \frac{ Y_{p} }{ (1-\sum_{i=1}^{p-1} \theta_{i} )^{2} } \\
\frac{ \partial ^{2} \ell }{\partial \theta_{i} \theta_{j}} & = -\frac{ Y_{p} }{ (1-\sum_{i=1}^{p-1}\theta_{i})^{2} }.
\end{align*}

This gives an information matrix with diagonals

$$
-\frac{ n }{ \theta_{i} } - \frac{ n }{ 1-\sum_{i=1}^{p-1} \theta_{i} }
$$
and off diagonals

$$
\frac{ n }{ 1-\sum_{i=1}^{p-1} \theta_{i} }.
$$

This can also be expressed as $I(\theta) = \text{diag}(1/p_{i}) + \pmb{1}\pmb{1}^{\top} / (1-\sum_{i=1}^{p-1} \theta_{i} )$.
<!-- This has known determinant $\mid I(\theta) \mid = \prod_{i=1}^{p-1} n / \theta_{i}$. -->


<!-- Then the Jeffreys prior is -->

<!-- \begin{align*} -->
<!-- \pi(\theta) & \propto \sqrt{ \mid I(\theta) \mid }  = \sqrt{ \prod_{i=1}^{p} \frac{ n }{ \theta_{i} } }  \propto \prod_{i=1}^{p} \frac{ 1 }{ \theta_{i}^{1/2} }  = \prod_{i=1}^{p} \theta^{\frac{ 1}{ 2 } - 1}. -->
<!-- \end{align*} -->
This has known determinant, giving a Jefferys prior of $\pi(\theta) = \prod_{i=1}^{p-1}\theta^{1-1/2}$. 
This is the PDF of a $\text{Dirichlet}(\alpha_{i} = 1/2)$ distribution.


**(b) Derive the posterior under the prior in (a).**

\begin{align*}
p(\pmb{\theta} \mid \pmb{Y}) & \propto \Big( \prod_{i=1}^{p} \theta_{i}^{Y_{i}} \Big) \Big( \prod_{i=1}^{p} \theta_{i}^{-1/2} \Big) \\
	& = \prod_{i=1}^{p} \theta_{i}^{Y_{i} - 1/2} 
\end{align*}

This is the PDF of a $\text{Dirichlet}(\alpha_{i} = Y_{i} + 1/2)$ distribution.

**(c) Assume that $\pmb{Y} = (10,20,30)$ and summarize the posterior under the prior in (a) with a figure and table.**



```{r, warning=FALSE, message=FALSE}
# use gtools for dirichlet draws
library(gtools)
library(ggplot2)
library(dplyr)
library(latex2exp)

set.seed(1978)
S = 10000

Y = c(10,20,30)
n = sum(Y)

alpha = Y + 1/2
posterior = rdirichlet(S, alpha)


posterior_df = data.frame(theta1 = posterior[,1],
													theta2 = posterior[,2],
													theta3 = posterior[,3])

ggplot(posterior_df) +
	geom_histogram(aes(x = theta1), alpha=0.5)	+
	xlim(0,1) +
	labs(x = TeX("$\\theta_{1}$"))

ggplot(posterior_df) +
	geom_histogram(aes(x = theta2), alpha=0.5)	+
	xlim(0,1) +
	labs(x = TeX("$\\theta_{2}$"))

ggplot(posterior_df) +
	geom_histogram(aes(x = theta3), alpha=0.5)	+
	xlim(0,1) +
	labs(x = TeX("$\\theta_{3}$"))

ggplot(posterior_df, aes(x=theta1, y=theta2) ) +
  geom_bin2d(bins = 100) +
  scale_fill_continuous(type = "viridis") +
  theme_bw()

summary_df = posterior_df  %>% summarise(
	theta1_mean = mean(theta1),
	theta1_sd = sd(theta1),
	theta1_lower = theta1_mean - qnorm(0.975) * theta1_sd,
	theta1_upper = theta1_mean + qnorm(0.975) * theta1_sd,
	theta2_mean = mean(theta2),
	theta2_sd = sd(theta2),
	theta2_lower = theta2_mean - qnorm(0.975) * theta2_sd,
	theta2_upper = theta2_mean + qnorm(0.975) * theta2_sd,
	theta3_mean = mean(theta3),
	theta3_sd = sd(theta3),
	theta3_lower = theta3_mean - qnorm(0.975) * theta3_sd,
	theta3_upper = theta3_mean + qnorm(0.975) * theta3_sd
)

```

|              | Posterior Mean | Posterior SD | 95% credible set |
|-------------:|---------:|-----------:|------:|
| $\theta_{1}$ |  0.170   |    0.047   |    (0.077, 0.263) |
| $\theta_{2}$ |  0.333   |    0.060   |    (0.216, 0.450) |
| $\theta_{3}$ |  0.458   |    0.064   |    (0.371, 0.622)

**(d) Now apply the Bayesian Central Limit Theorem to obtain an approximate normal distribution for the posterior of $\theta$ given $\pmb{Y} = (10,20,30)$. Summarize this approximate posterior in a figure and table. Are the results similar to the exact posterior? Is this a good approximation?**

We need to find the Fisher information of the posterior with the condition $\sum_{i=1}^{3} \theta_{i} = 1$. Take $\ell$ to the the log posterior,

\begin{align*}
\ell & = (Y_{1} - \frac{ 1 }{ 2 }) \log(\theta_{1}) + (Y_{2} - \frac{ 1 }{ 2 }) \log(\theta_{2}) + (Y_{3} - \frac{ 1 }{ 2 }) \log(1 - \theta_{1} - \theta_{2}) \\ \\
%
\frac{ \partial \ell }{\partial \theta_{1}} & = \frac{ Y_{1} - \frac{ 1 }{ 2 } }{ \theta_{1} } - \frac{ Y_{3} - \frac{ 1 }{ 2 } }{ 1 - \theta_{1} - \theta_{2} } \\
\frac{ \partial \ell }{\partial \theta_{2}} & = \frac{ Y_{2} - \frac{ 1 }{ 2 } }{ \theta_{2} } - \frac{ Y_{3} - \frac{ 1 }{ 2 } }{ 1 - \theta_{1} - \theta_{2} } \\ \\
% 
\frac{ \partial^{2} \ell }{\partial \theta_{1}^{2}} & = -\frac{ Y_{1} - \frac{ 1 }{ 2 } }{ \theta_{1}^{2]} } - \frac{ Y_{3} - \frac{ 1 }{ 2 } }{ (1 - \theta_{1} - \theta_{2})^{2} } \\
\frac{ \partial^{2} \ell }{\partial \theta_{2}^{2}} & = -\frac{ Y_{2} - \frac{ 1 }{ 2 } }{ \theta_{2}^{2} } - \frac{ Y_{3} - \frac{ 1 }{ 2 } }{ (1 - \theta_{1} - \theta_{2})^{2} } \\
\frac{ \partial^{2} \ell }{\partial \theta_{1} \theta_{2}} & = - \frac{ Y_{3} - \frac{ 1 }{ 2 } }{ (1 - \theta_{1} - \theta_{2})^{2} }.
\end{align*}

Then our Fisher information looks like

$$
I = \begin{bmatrix}
\frac{ n\theta_{1} - \frac{ 1 }{ 2 } }{ \theta_{1}^{2} } + \frac{ n(1-\theta_{1} - \theta_{2}) - \frac{ 1 }{ 2 } }{ (1-\theta_{1} - \theta_{2})^{2} } & -\frac{ n(1-\theta_{1} - \theta_{2}) - \frac{ 1 }{ 2 } }{ (1-\theta_{1} - \theta_{2})^{2} } \\
 \frac{ n(1-\theta_{1} - \theta_{2}) - \frac{ 1 }{ 2 } }{ (1-\theta_{1} - \theta_{2})^{2} } & \frac{ n\theta_{2} - \frac{ 1 }{ 2 } }{ \theta_{2}^{2} } + \frac{ n(1-\theta_{1} - \theta_{2}) - \frac{ 1 }{ 2 } }{ (1-\theta_{1} - \theta_{2})^{2} }
\end{bmatrix}.
$$

Note we take $\theta_{1,0} = (Y_{1}-1/2) / (Y_{1} + Y_{2} - 1)$ and  $\theta_{2,0} = (Y_{2}-1/2) / (Y_{1} + Y_{2} - 1)$.

```{r, message=FALSE, warning = FALSE}
library(MASS)
library(gtools)
library(ggplot2)
library(dplyr)
library(latex2exp)

set.seed(1978)
S = 10000

Y = c(10,20,30)
n = sum(Y)
theta10 = (Y[1] - 1/2) / (Y[1] + Y[2] +Y[3] -3/2)
theta20 = (Y[2] - 1/2) / (Y[1] + Y[2] +Y[3] -3/2)

mu = c( theta10, theta20)
I = matrix(c(
	(n * theta10 - 1/2) / (theta10^2), 0,
	0, (n * theta20 - 1/2) / (theta20^2)
), 2, 2) + ( n*( 1 - theta10 - theta20) - 1/2 ) / ((1 - theta10 - theta20)^2)


posterior = mvrnorm(S, mu, solve(I))


posterior_df = data.frame(theta1 = posterior[,1], theta2 = posterior[,2])

ggplot(posterior_df, aes(x=theta1, y=theta2) ) +
  geom_bin2d(bins = 100) +
  scale_fill_continuous(type = "viridis") +
  theme_bw()


summary_df = posterior_df  %>% summarise(
	theta1_mean = mean(theta1),
	theta1_sd = sd(theta1),
	theta1_lower = theta1_mean - qnorm(0.975) * theta1_sd,
	theta1_upper = theta1_mean + qnorm(0.975) * theta1_sd,
	theta2_mean = mean(theta2),
	theta2_sd = sd(theta2),
	theta2_lower = theta2_mean - qnorm(0.975) * theta2_sd,
	theta2_upper = theta2_mean + qnorm(0.975) * theta2_sd
)


```


|              | Asymp Mean | Asymp SD | 95% credible set |
|-------------:|---------:|-----------:|------:|
| $\theta_{1}$ |  0.169   |    0.048   |    (0.067, 0.257) |
| $\theta_{2}$ |  0.335   |    0.061   |    (0.214, 0.455) |

The asymptotic means and standard deviations are similar to that of the posterior without asymptotics. This is a good approximation!

\newpage

**3. Assume that $Y_{i}\mid \theta \sim \text{Uniform}(0,\theta)$ independent for $i \in \{ 1, \dots , n \}$ and prior $\theta \sim \text{Pareto}(\theta_{0}, \alpha)$ with support $\theta > \theta_{0}$ and CDF $\text{Prob}(\theta < t) = 1 - (\theta_{0} / t)^{\alpha}$.**

**(a) Say the true value of $\theta$ is $\theta^{*} = 10$ and the prior has $\theta_{0} = \alpha = 1$. For a dataset of size $n$, $\pmb{Y} = \{Y_{1}, \dots ,Y_{n} \}$, let $p_{n} = E_{\pmb{Y}_{n} \mid \theta^{*}}\{ \text{Prob}(\theta^{*} - \epsilon < \theta < \theta^{*} + \epsilon \mid \pmb{Y}_{n}) \}$ for $\epsilon = 0.1$. Compute a Monte Carlo approximation to $p_{n}$ for each $n \in \{10, 20, \dots , 1000\}$. Does a plot of $n$ versus $p_{n}$ suggest posterior consistency? Why?**


```{r, warning=FALSE, message=FALSE}
# use for pareto distribution
library("EnvStats")
set.seed(1978)
theta_star = 10
theta0 = alpha = 1
epsilon = 0.1
S = 10000

n = seq(10, 1000, 10)
pn = sapply(n, function(n){
	Y = runif(n, 0, theta_star)
	theta_posterior = rpareto(S, max(max(Y), theta0), n + alpha)
	return(mean(theta_star - epsilon < theta_posterior & theta_posterior < theta_star + epsilon))
})

pn_df = data.frame(pn = pn, n = n)

ggplot(pn_df, aes(x=n, y=pn)) + geom_line()
```



**(b) Without evoking any general theorems discussed in class, derive $\lim_{n \rightarrow \infty }p_{n}$. Do you get the same conclusion about posterior consistency as the Monte Carlo study in (a)?**

In homework 1 we found that the posterior distribution for this likelihood and prior is $\theta \mid Y_{1}, \dots Y_{n} \sim \text{Pareto}(\max(y_{(n)}, \theta_{0}), n + \alpha)$.
Then the probability statement inside of the expectation looks like

\begin{align*}
\text{Prob}& (\theta^{*} - \varepsilon < \theta < \theta^{*} + \varepsilon \mid \pmb{Y}_{n}) = F_{\theta \mid \pmb{Y}}(\theta^{*} + \varepsilon) - F_{\theta \mid \pmb{Y}}(\theta^{*} - \varepsilon) \\
	& = \Big( 1- \Big(\frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} + \varepsilon } \Big)^{n+\alpha}\Big)\mathbb{I}(\theta^{*} + \varepsilon > \max(Y_{(n)}, \theta_{0})) - \Big( 1-\Big(\frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} - \varepsilon } \Big)^{n+\alpha}\Big) \mathbb{I}(\theta^{*} - \varepsilon > \max(Y_{(n)}, \theta_{0}))
\end{align*}

Now we take the expectation of the first part.
Notice that the indicator is always 1.

\begin{align*}
E & \Big[ \Big( 1- \frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} + \varepsilon } \Big)^{n+\alpha} \Big] \\
	& = E\Big[ 1 - \Big( \frac{ Y_{(n)} }{ \theta^{*} + \varepsilon } \Big)^{n + \alpha} \Big] P(Y_{(n)} > \theta_{0}) + \Big[ 1 - \Big( \frac{ \theta_{0} }{ \theta^{*} + \varepsilon } \Big)^{n + \alpha}\Big] P(Y_{(n)} < \theta_{0}) \\
	& = \Big[ 1 - \frac{ 1 }{ (\theta^{*} + \varepsilon)^{n+\alpha} }\int_{0}^{\theta^{*}} y^{n+\alpha} \frac{ n y^{n-1} }{ \theta^{* n} } dy \Big] \Big[ 1- \Big(\frac{ \theta_{0} }{ \theta_{*} }\Big)^{n} \Big] 
		+ \Big[ 1 - \Big( \frac{ \theta_{0} }{ \theta^{*} + \varepsilon } \Big)^{n + \alpha}\Big] \Big(\frac{ \theta_{0} }{ \theta^{*} }\Big)^{n} \\
% 
	& = \Big[ 1 - \frac{  \theta^{*(2n+\alpha)} }{ \theta^{*n}(\theta^{*} + \varepsilon)^{n+\alpha} } \frac{n }{ 2n+\alpha } \Big] \Big[ 1- \Big(\frac{ \theta_{0} }{ \theta_{*} }\Big)^{n} \Big] 
		+ \Big[ 1 - \Big( \frac{ \theta_{0} }{ \theta^{*} + \varepsilon } \Big)^{n + \alpha}\Big] \Big(\frac{ \theta_{0} }{ \theta^{*} }\Big)^{n} \\
\end{align*}

Taking the limit of this gives

\begin{align*}
\lim_{n\rightarrow \infty}E & \Big[ \Big( 1- \frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} + \varepsilon } \Big)^{n+\alpha} \Big] \\
	& = \Big[ 1 - 0 \cdot \frac{ 1 }{ 2 } \Big] \Big[ 1-0 \Big] + \Big[ 1-0 \Big] 0 \\
	& = 1.
\end{align*}

Now we focus on the second expectation

\begin{align*}
E & \Big[ \Big( 1- \frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} - \varepsilon } \Big)^{n+\alpha} \mathbb{I}(\theta^{*} - \varepsilon > \max(Y_{(n)}, \theta_{0})) \Big] \\
& = E  \Big[ \Big( 1- \frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} - \varepsilon } \Big)^{n+\alpha}  \mid \theta^{*} - \varepsilon > \max(Y_{(n)}, \theta_{0}) \Big] P(\theta^{*} - \varepsilon > \max(Y_{(n)}, \theta_{0})) \\
	& + E  \Big[ \Big( 1- \frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} - \varepsilon } \Big)^{n+\alpha}  \mid \theta^{*} - \varepsilon < \max(Y_{(n)}, \theta_{0}) \Big] P(\theta^{*} - \varepsilon < \max(Y_{(n)}, \theta_{0})) \\
	& = E  \Big[ \Big( 1- \frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} - \varepsilon } \Big)^{n+\alpha}  \mid \theta^{*} - \varepsilon > \max(Y_{(n)}, \theta_{0}) \Big] P(\theta^{*} - \varepsilon > \max(Y_{(n)}, \theta_{0})) + 0.
\end{align*}

Where the last simplification comes from the fact that the CDF of the Pareto distribution is 0 on that interval.
Now we can integrate,

\begin{align*}
E & \Big[ \Big( 1- \frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} - \varepsilon } \Big)^{n+\alpha} \mathbb{I}(\theta^{*} - \varepsilon > \max(Y_{(n)}, \theta_{0})) \Big] \\
	& = E\Big[ 1 - \Big( \frac{ Y_{(n)} }{ \theta^{*} + \varepsilon }\Big)^{n + \alpha} \mid \theta^{*} - \varepsilon > Y_{(n)} \Big] P(Y_{(n)} > \theta_{0}) + \Big[ 1 - \Big( \frac{ \theta_{0} }{ \theta^{*} - \varepsilon } \Big)^{n + \alpha}\Big] P(Y_{(n)} < \theta_{0}) \\
	& = \Big[ 1 - \frac{  (\theta^{*}-\varepsilon)^{(2n+\alpha)} }{ \theta^{*n}(\theta^{*} + \varepsilon)^{n+\alpha} } \frac{n }{ 2n+\alpha } \Big] \Big[ 1- \Big(\frac{ \theta_{0} }{ \theta_{*} }\Big)^{n} \Big] 
		+ \Big[ 1 - \Big( \frac{ \theta_{0} }{ \theta^{*} - \varepsilon } \Big)^{n + \alpha}\Big] \Big(\frac{ \theta_{0} }{ \theta^{*} }\Big)^{n} \\
		& \rightarrow \Big[ 1 - 0 \cdot \frac{ 1 }{ 2 } \Big] \Big[ 1-0 \Big] + \Big[ 1 - 0 \Big] 1
\end{align*}

<!-- We can break the second expectation up over  -->

Additionally, since $Y_{(n)}$ is consistent for $\theta^{*}$, we know that $\lim_{n\rightarrow \infty} P(\frac{ \max(Y_{(n)} ,\theta_{0}) }{ \theta^{*}-\varepsilon } < 1) = 0$ because the definition of convergence states that for some $N$, $Y_{(n)}$ will be within $\varepsilon$ of $\theta^{*}$.
Thus, $p_{n} \rightarrow 1 - 0 = 1$.

<!-- \begin{align*} -->
<!-- E & \Big[ \text{Prob}(\theta^{*} - \varepsilon < \theta < \theta^{*} + \varepsilon \mid \pmb{Y}_{n}) \Big] \\ -->
<!-- 	& = E \Big[ \Big( \frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} - \varepsilon } \Big)^{n+\alpha}\mathbb{I}(\theta^{*} - \varepsilon > \max(Y_{(n)}, \theta_{0})) \Big] - E \Big[  \Big( \frac{ \max(Y_{(n)}, \theta_{0}) }{ \theta^{*} + \varepsilon } \Big)^{n+\alpha} \mathbb{I}(\theta^{*} + \varepsilon > \max(Y_{(n)}, \theta_{0})) \Big] \\ \\ -->
<!-- 	& = \frac{ 1 }{ (\theta^{*} - \varepsilon)^{n+\alpha} }  -->
<!-- 	    \Big\{  -->
<!-- 	        E \Big[ Y_{(n)}^{n+\alpha} \mathbb{I}(Y_{(n)} > \theta_{0}) \mathbb{I}(\theta^{*} - \varepsilon > Y_{(n)}) \Big] -->
<!-- 	        + E \Big[ \theta_{0}^{n+\alpha} \mathbb{I}(Y_{(n)}<  \theta_{0}) \mathbb{I}(\theta^{*} - \varepsilon > \theta_{0}) \Big] -->
<!-- 	        \Big\} \\ -->
<!--     & \hspace{0.5cm} - \frac{ 1 }{ (\theta^{*} + \varepsilon)^{n+\alpha} }  -->
<!-- 	    \Big\{  -->
<!-- 	        E \Big[ Y_{(n)}^{n+\alpha} \mathbb{I}(Y_{(n)} > \theta_{0}) \mathbb{I}(\theta^{*} + \varepsilon > Y_{(n)}) \Big] -->
<!-- 	        + E \Big[ \theta_{0}^{n+\alpha} \mathbb{I}(Y_{(n)}<  \theta_{0}) \mathbb{I}(\theta^{*} + \varepsilon > \theta_{0}) \Big] -->
<!-- 	        \Big\} \\ \\ -->
<!-- %  -->
<!-- 	& = \frac{ 1 }{ (\theta^{*} - \varepsilon)^{n+\alpha} }  -->
<!-- 	    \Big\{  -->
<!-- 	        \int_{\theta_{0}}^{\theta^{*} - \varepsilon} y^{n+\alpha} \frac{ n y^{n-1} }{ \theta^{*n} } dy  -->
<!-- 	        + \int_{0}^{\theta_{0}} \theta_{0}^{n+\alpha} \frac{ n y^{n-1} }{ \theta^{*n} } dy  -->
<!--         \Big\} -->
<!-- 	         \\ -->
<!--     & \hspace{0.5cm} - \frac{ 1 }{ (\theta^{*} + \varepsilon)^{n+\alpha} }  -->
<!-- 	    \Big\{  -->
<!-- 	       \int_{\theta_{0}}^{\theta^{*} + \varepsilon} y^{n+\alpha} \frac{ n y^{n-1} }{ \theta^{*n} } dy  -->
<!-- 	        + \int_{0}^{\theta_{0}} \theta_{0}^{n+\alpha} \frac{ n y^{n-1} }{ \theta^{*n} } dy  -->
<!-- 	        \Big\} \\ \\ -->
<!-- %  -->
<!-- 	& = \frac{ 1 }{ \theta^{*n} (\theta^{*} - \varepsilon)^{n+\alpha} } -->
<!-- 	    \Big\{ -->
<!-- 	        \frac{ n }{ 2n + \alpha } (\theta^{*} - \varepsilon)^{2n+\alpha} - \frac{ n }{ 2n + \alpha } \theta_{0}^{2n + \alpha} + \frac{ n }{ n } \theta_{0}^{2n+\alpha} -->
<!--         \Big\} -->
<!-- 	         \\ -->
<!--     & \hspace{0.5cm} - \frac{ 1 }{ \theta^{*n} (\theta^{*} + \varepsilon)^{n+\alpha} } -->
<!-- 	    \Big\{ -->
<!-- 	        \frac{ n }{ 2n + \alpha } (\theta^{*} + \varepsilon)^{2n+\alpha} - \frac{ n }{ 2n + \alpha } \theta_{0}^{2n + \alpha} + \frac{ n }{ n } \theta_{0}^{2n+\alpha} -->
<!--         \Big\} \\ \\ -->
<!-- %  -->
<!-- 	& = \frac{ n }{ 2n + \alpha } \frac{ (\theta^{*} - \varepsilon)^{2n+\alpha} }{ \theta^{*} (\theta^{*} - \varepsilon)^{n+\alpha} }  -->
<!-- 	    + \Big( 1 - \frac{ n }{ 2n + \alpha }\Big) \frac{ \theta_{0}^{2n + \alpha} }{ \theta^{*n} (\theta^{*} - \varepsilon)^{n + \alpha} } -->
<!-- 	    - \frac{ n }{ 2n + \alpha } \frac{ (\theta^{*} + \varepsilon)^{2n+\alpha} }{ \theta^{*} (\theta^{*} + \varepsilon)^{n+\alpha} }  -->
<!-- 	    - \Big( 1 - \frac{ n }{ 2n + \alpha }\Big) \frac{ \theta_{0}^{2n + \alpha} }{ \theta^{*n} (\theta^{*} + \varepsilon)^{n + \alpha} } \\ -->
<!--     & \rightarrow_{n\rightarrow\infty} \frac{ 1 }{ 2 } \cdot 1  -->
<!--         + (1 - \frac{ 1 }{ 2 }) \cdot 0  -->
<!--         - \frac{ 1 }{ 2 } \cdot 1 -->
<!--         - (1 - \frac{ 1 }{ 2 }) \cdot 0 \\  -->
<!--     & = 0 -->
<!-- \end{align*} -->